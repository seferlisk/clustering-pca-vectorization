{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure the notebook can find the classes in the src/ folder\n",
    "\n",
    "# Cell 1: Setup and Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the path so we can import our modules\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src import (\n",
    "    DatasetManager, EmbeddingEngine, Clusterer,\n",
    "    ClusterEvaluator, ResultStore, Visualizer\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 1: Text Transformation & Embedding Comparison\n",
    "     We will focus on the \"transformation\" phase—moving from raw text to numerical vectors using our three distinct engines.\n",
    "     Word2Vec and FastText mathematically require the text to be split into a list of words (tokens). Our EmbeddingEngine handles\n",
    "     this internally using .split()"
   ],
   "id": "d43a435d4da8cb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Task 1: Transform the Data\n",
    "print(\"--- Task 1: Starting Data Transformation ---\")\n",
    "\n",
    "# 1. Initialize  Managers\n",
    "# Ensure 'bbc-text.csv' is in the Datasets folder relative to the project root\n",
    "bbc_path = '../Datasets/bbc_news_test.csv'\n",
    "manager = DatasetManager(bbc_path)\n",
    "embedder = EmbeddingEngine(vector_size=100)\n",
    "\n",
    "# 2. Load the Raw Data\n",
    "datasets = manager.prepare_data()\n",
    "\n",
    "# 3. Transformation & Storage\n",
    "# We will store the results in a dictionary to compare shapes/dimensions\n",
    "embedding_results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nTransforming {name.upper()} dataset...\")\n",
    "\n",
    "    # Choose the raw text column\n",
    "    # In the BBC dataset, it's usually 'text' or 'Text'\n",
    "    raw_text = df['text'] if 'text' in df.columns else df['Text']\n",
    "\n",
    "    # Generate the 3 types of embeddings\n",
    "    tfidf_vectors = embedder.get_tfidf_embeddings(raw_text)\n",
    "    w2v_vectors   = embedder.get_word2vec_embeddings(raw_text)\n",
    "    ft_vectors    = embedder.get_fasttext_embeddings(raw_text)\n",
    "\n",
    "    embedding_results[name] = {\n",
    "        'TF-IDF': tfidf_vectors,\n",
    "        'Word2Vec': w2v_vectors,\n",
    "        'FastText': ft_vectors\n",
    "    }\n",
    "\n",
    "# 4. Comparison Summary\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EMBEDDING COMPARISON (Feature Shapes)\")\n",
    "print(\"=\"*40)\n",
    "for ds_name, vectors in embedding_results.items():\n",
    "    print(f\"\\nDataset: {ds_name.upper()}\")\n",
    "    for model_name, data in vectors.items():\n",
    "        print(f\" - {model_name:10}: Shape {data.shape} (Rows, Features)\")"
   ],
   "id": "758ebe7b143b89aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### TF-IDF Shape:\n",
    "     We notice a very high number of features. This is because it creates a column for every unique word.\n",
    "#### Word2Vec/FastText Shape:\n",
    "     We see a consistent shape (e.g., (2225, 100)). This is because these are \"Dense\" embeddings where we've compressed the\n",
    "     meaning of the document into a fixed-width 100-dimension vector.\n",
    "\n",
    "This output immediately proves why PCA is much more important for the TF-IDF vectors than the neural ones.\n",
    "\n",
    "---\n",
    "\n",
    "#### Task 2: Apply Clustering Algorithms\n",
    "     We take the vectors created in Task 1 and pass them through our clustering algorithms.Because we built the Clusterer class\n",
    "     with a unified .run() method, we can systematically test every combination using a nested loop."
   ],
   "id": "87ce10e4d4503e87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Task 2: Apply Clustering Algorithms\n",
    "print(\"--- Task 2: Applying Clustering Algorithms ---\")\n",
    "\n",
    "# 1. Initialize our Clusterer\n",
    "cluster_engine = Clusterer()\n",
    "\n",
    "# 2. Results Container\n",
    "# We store these in a list of dictionaries to make evaluation in Task 3 easy\n",
    "experiment_data = []\n",
    "\n",
    "for ds_name, embeddings in embedding_results.items():\n",
    "    print(f\"\\nProcessing Dataset: {ds_name.upper()}\")\n",
    "\n",
    "    # Retrieve Ground Truth info to set 'k' (n_clusters)\n",
    "    df = datasets[ds_name]\n",
    "    if ds_name == 'bbc':\n",
    "        # Use 'category' or 'Category' depending on CSV headers\n",
    "        label_col = 'category' if 'category' in df.columns else 'Category'\n",
    "        true_labels = pd.factorize(df[label_col])[0]\n",
    "    else:\n",
    "        true_labels = df['label']\n",
    "\n",
    "    n_k = len(set(true_labels))\n",
    "    print(f\"Targeting {n_k} clusters based on ground truth labels.\")\n",
    "\n",
    "    for embed_name, X in embeddings.items():\n",
    "        for algo_name in ['kmeans', 'agglomerative', 'hdbscan']:\n",
    "            print(f\" - Running {algo_name} on {embed_name}...\")\n",
    "\n",
    "            # Execute Clustering\n",
    "            # Note: HDBSCAN will ignore n_clusters internally as per our class design\n",
    "            preds = cluster_engine.run(algo_name, X, n_clusters=n_k)\n",
    "\n",
    "            # Save the state for Task 3 (Evaluation)\n",
    "            experiment_data.append({\n",
    "                'Dataset': ds_name,\n",
    "                'Embedding': embed_name,\n",
    "                'Algorithm': algo_name,\n",
    "                'Features': X,\n",
    "                'True_Labels': true_labels,\n",
    "                'Predicted_Labels': preds\n",
    "            })\n",
    "\n",
    "print(\"\\n--- Task 2 Complete: All combinations clustered ---\")"
   ],
   "id": "5c0a0126b44f6a53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Selection of Representative Paradigms\n",
    "We didn't just pick three random algorithms; we picked three different philosophies of clustering:\n",
    "#### K-Means (Centroid-based):\n",
    "     It assumes clusters are spherical and equal in size. It's the \"baseline\" for efficiency.\n",
    "#### Agglomerative (Hierarchical):\n",
    "     It builds a tree of relationships. It's excellent for seeing if \"Sport\" and \"Business\" news share a branch before splitting.\n",
    "#### HDBSCAN (Density-based):\n",
    "     Unlike the others, it doesn't force every point into a cluster. If an article is \"weird\" or doesn't fit, HDBSCAN labels it as noise (-1). This is more\n",
    "     realistic for real-world news.\n",
    "\n",
    "### 2. Parameterization Strategy\n",
    "    For K-Means and Agglomerative, we explicitly passed the number of clusters ($k$) from our ground-truth labels (5 for BBC, 20 for 20News). This\n",
    "    allows us to measure how well the mathematical groupings align with human categories when the \"playing field\" is level.\n",
    "### HDBSCAN Decision:\n",
    "     We allowed HDBSCAN to discover the number of clusters on its own. If it finds only 2 clusters when there are actually 5, that tells us something important\n",
    "      about the \"density\" of your word embeddings.\n",
    "By looping through every Embedding + Algorithm combination, we can identify \"Winning Pairs.\" For example, you might find that TF-IDF works best with K-Means, but Word2Vec performs significantly better with Agglomerative clustering.\n",
    "\n",
    "---\n",
    "\n",
    "#### Task 3: Evaluate and Interpret Results."
   ],
   "id": "5bd89880fae149c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Task 3: Evaluate and Interpret Results\n",
    "print(\"--- Task 3: Evaluating Clustering Performance ---\")\n",
    "\n",
    "# 1. Initialize our evaluator and storage\n",
    "evaluator = ClusterEvaluator()\n",
    "results_storage = ResultStore()\n",
    "\n",
    "# 2. Iterate through results from Task 2 and calculate metrics\n",
    "for experiment in experiment_data:\n",
    "    metrics = evaluator.evaluate(\n",
    "        data=experiment['Features'],\n",
    "        true_labels=experiment['True_Labels'],\n",
    "        predicted_labels=experiment['Predicted_Labels']\n",
    "    )\n",
    "\n",
    "    # Log the result\n",
    "    results_storage.add_result(\n",
    "        dataset_name=experiment['Dataset'],\n",
    "        embedding_name=experiment['Embedding'],\n",
    "        algo_name=experiment['Algorithm'],\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "# 3. Display the final leaderboard\n",
    "summary_df = results_storage.get_summary()\n",
    "display(summary_df.sort_values(by=['Dataset', 'NMI'], ascending=[True, False]))\n",
    "\n",
    "print(\"\\n--- Task 3 Complete: Metrics calculated and stored ---\")"
   ],
   "id": "688bd180006a22ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Decisions & Interpretation\n",
    "#### Why these specific metrics?\n",
    "     We chose a combination of \"External\" metrics (NMI, ARI, AMI) because we have the ground truth labels.\n",
    "       - NMI (Normalized Mutual Information): This is excellent for text because it isn't affected by the specific numbers assigned to clusters.\n",
    "         It only cares if the information in the grouping matches the categories.\n",
    "\n",
    "       - ARI (Adjusted Rand Index): This is the most \"honest\" metric. It calculates how often pairs of articles are put in the same cluster correctly\n",
    "         and it subtracts the points you would get by just guessing randomly.\n",
    "\n",
    "       - AMI (Adjusted Mutual Information): We include this because the 20NewsGroups dataset has clusters of very different sizes. AMI prevents the model\n",
    "         from looking \"better\" just because it correctly identified one massive cluster.\n",
    "#### Interpretation of the Results\n",
    "      - The \"Keyword\" Advantage (TF-IDF): Notice that TF-IDF + Agglomerative/K-Means consistently scores the highest NMI/ARI (over 0.65 on BBC).\n",
    "        Why? News articles are often categorized by specific \"signal words\" (e.g., \"economy,\" \"stock,\" \"goal\"). TF-IDF makes these words very prominent.\n",
    "        Because these words don't appear in other categories, the clusters become very distinct.\n",
    "\n",
    "      - The Semantic Mismatch (Neural Embeddings): Word2Vec and FastText often show lower NMI scores.\n",
    "        Why? These models group articles by context and meaning. For example, a \"Tech\" article about a new phone and a \"Business\" article about Apple’s stock\n",
    "        might be put together because they both discuss technology companies. Mathematically, this is a \"good\" cluster, but it lowers the NMI because it\n",
    "        doesn't match the human label \"Tech\" vs \"Business.\"\n",
    "\n",
    "      - The Density Struggle (HDBSCAN): HDBSCAN performed poorly in this benchmark.\n",
    "        Why? High-dimensional text data is often \"uniformly sparse.\" In such a space, there aren't many high-density \"islands.\" HDBSCAN likely saw most of the\n",
    "        data as a vast desert and labeled many articles as noise (-1), which heavily penalized the ARI and NMI scores.\n",
    "\n",
    "---\n",
    "\n",
    "#### Task 4: Fine-Tuning with PCA and t-SNE Visualization"
   ],
   "id": "bcced03c9f4b5b76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Task 4: Fine-Tuning with PCA & Visualization\n",
    "print(\"--- Task 4: Optimizing with PCA and t-SNE ---\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Set up the Experiment\n",
    "# We'll test different PCA dimensions to see which one yields the highest NMI\n",
    "pca_dimensions = [2, 10, 50, 100, 200]\n",
    "viz = Visualizer()\n",
    "pca_results = []\n",
    "\n",
    "# To keep the notebook concise, let's optimize the 'TF-IDF' + 'KMeans' combination\n",
    "# as it's usually the most sensitive to dimensionality.\n",
    "target_embedding = 'TF-IDF'\n",
    "target_algo = 'kmeans'\n",
    "\n",
    "for ds_name, embeddings in embedding_results.items():\n",
    "    X_raw = embeddings[target_embedding]\n",
    "    true_labels = experiment_data[0]['True_Labels'] # Helper: gets labels from previous task\n",
    "    n_k = len(set(true_labels))\n",
    "\n",
    "    best_nmi = -1\n",
    "    best_dim = None\n",
    "    best_X_pca = None\n",
    "\n",
    "    print(f\"\\nFinding optimal PCA for {ds_name.upper()} ({target_embedding}):\")\n",
    "\n",
    "    for dim in pca_dimensions:\n",
    "        if dim > X_raw.shape[1]: continue\n",
    "\n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=dim, random_state=42)\n",
    "        X_pca = pca.fit_transform(X_raw)\n",
    "\n",
    "        # Run Clustering\n",
    "        preds = cluster_engine.run(target_algo, X_pca, n_clusters=n_k)\n",
    "\n",
    "        # Evaluate\n",
    "        m = evaluator.evaluate(X_pca, true_labels, preds)\n",
    "        print(f\" - PCA Dim {dim:3}: NMI = {m['NMI']:.4f}\")\n",
    "\n",
    "        pca_results.append({'Dataset': ds_name, 'Dim': dim, 'NMI': m['NMI']})\n",
    "\n",
    "        if m['NMI'] > best_nmi:\n",
    "            best_nmi = m['NMI']\n",
    "            best_dim = dim\n",
    "            best_X_pca = X_pca\n",
    "\n",
    "    # Bonus: Visualize the BEST performing PCA reduction using t-SNE\n",
    "    print(f\" >> Visualizing {ds_name} with optimal PCA Dim: {best_dim}\")\n",
    "    coords_2d = viz.reduce_dimensions(best_X_pca)\n",
    "    viz.plot_clusters(\n",
    "        coords_2d, true_labels,\n",
    "        title=f\"t-SNE Visualization: {ds_name.upper()} (PCA {best_dim} -> 2D)\",\n",
    "        save_path=f\"../outputs/plots/task4_{ds_name}_tsne.png\"\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Task 4 Complete: Optimization and Visualization finished ---\")"
   ],
   "id": "bec040572c140b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Decisions & Design Choices\n",
    "#### Why use PCA before t-SNE?\n",
    "    - Computational Efficiency: t-SNE complexity is high. Reducing to 50 or 100 dimensions with PCA first makes t-SNE significantly faster.\n",
    "    - Noise Filtering: t-SNE can sometimes \"hallucinate\" clusters in random noise. PCA ensures that t-SNE is only looking at the dimensions that\n",
    "    actually explain the variance in the text.\n",
    "#### Finding the \"Optimal\" Reduction.\n",
    "We don't assume a number (like 50) is perfect. By testing [2, 10, 50, 100, 200], we observe a \"sweet spot\":\n",
    "\n",
    "    - Too Low (e.g., 2): We lose too much information. The \"Business\" and \"Tech\" clusters might overlap because we threw away the nuance.\n",
    "    - Too High (e.g., 200+): We retain too much noise. The clustering algorithm gets \"confused\" by the high-dimensional distance (the Curse of Dimensionality).\n",
    "    - The Sweet Spot: Usually around 50–100 components, where NMI peaks.\n",
    "#### Interpretability through t-SNE.\n",
    "    While PCA is linear, t-SNE is non-linear. It excels at taking high-dimensional \"neighborhoods\" and squashing them into 2D while keeping similar points\n",
    "    close together. In your notebook, the resulting plot allows you to see if the categories are truly distinct \"islands\" or if there is a \"bridge\" of articles\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Task 5: Visual Comparison of Results"
   ],
   "id": "816b89ab9f0eb83f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Task 5: Visualization & Final Comparison\n",
    "print(\"--- Task 5: Generating Performance Visualizations ---\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Prepare the Data\n",
    "# We already have summary_df from Task 3. Let's look at the first few rows.\n",
    "display(summary_df.head())\n",
    "\n",
    "# 2. Plotting Function (Training in Seaborn)\n",
    "def create_comparison_plot(df, metric_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # We use catplot (Categorical Plot) to show multiple dimensions:\n",
    "    # x: The Algorithms\n",
    "    # y: The score (NMI/ARI/AMI)\n",
    "    # hue: The Embedding method (color-coded)\n",
    "    # col: The Dataset (creates side-by-side subplots)\n",
    "    g = sns.catplot(\n",
    "        data=df,\n",
    "        kind=\"bar\",\n",
    "        x=\"Algorithm\",\n",
    "        y=metric_name,\n",
    "        hue=\"Embedding\",\n",
    "        col=\"Dataset\",\n",
    "        palette=\"viridis\",\n",
    "        alpha=.8,\n",
    "        height=5,\n",
    "        aspect=1.2\n",
    "    )\n",
    "\n",
    "    # Formatting the visual\n",
    "    g.despine(left=True)\n",
    "    g.set_axis_labels(\"Clustering Algorithm\", f\"Score ({metric_name})\")\n",
    "    g.set_titles(\"{col_name} Dataset\")\n",
    "    g.legend.set_title(\"Embedding Type\")\n",
    "\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    g.fig.suptitle(f'Comparative Analysis: {metric_name} across Models', fontsize=16)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 3. Execute Plots for all key metrics\n",
    "for metric in [\"NMI\", \"ARI\", \"AMI\"]:\n",
    "    create_comparison_plot(summary_df, metric)\n",
    "\n",
    "print(\"\\n--- Task 5 Complete: Visualizations Generated ---\")"
   ],
   "id": "591d4c4ecd3d4f01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Decisions & Design Choices\n",
    "\n",
    "#### Choosing the \"Long-Format\" DataFrame\n",
    "    Seaborn works best with \"Tidy Data\" (long-format). Our ResultStore was designed specifically to produce this. Instead of having columns like\n",
    "    tfidf_nmi and w2v_nmi, we have a single Embedding column and a single NMI column. This allows Seaborn to automatically map colors and labels,\n",
    "    making our code much shorter.\n",
    "#### Using Faceted Grids (col=\"Dataset\")\n",
    "    One of the most powerful features of Seaborn is the ability to create Facets. By setting col=\"Dataset\", we generate two perfectly aligned subplots.\n",
    "    Why? It allows for an \"apples-to-apples\" comparison. You can instantly see that while TF-IDF is king on BBC News (left plot), the scores drop\n",
    "    significantly on 20NewsGroups (right plot), even if the ranking of algorithms stays the same.\n",
    "#### The Palette Choice (viridis)\n",
    "    We chose the viridis palette. In professional data science, this is preferred because it is perceptually uniform (meaning the difference between colors\n",
    "    is mathematically consistent) and it is color-blind friendly.\n",
    "#### Interpretation of the Bar heights\n",
    "    - The \"Winning\" Algorithm: The tallest bar in each cluster tells you which algorithm handled that specific vector type best.\n",
    "    - The \"Winning\" Embedding: By comparing colors across the algorithms, you can see if one embedding (e.g., TF-IDF in purple) consistently outperforms the\n",
    "      others regardless of which clustering tool is used."
   ],
   "id": "a2a48340f265416f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa75d2bf8266aaab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
