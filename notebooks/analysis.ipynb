{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure the notebook can find the classes in the src/ folder\n",
    "\n",
    "# Cell 1: Setup and Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the path so we can import our modules\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src import (\n",
    "    DatasetManager, EmbeddingEngine, Clusterer,\n",
    "    ClusterEvaluator, ResultStore, Visualizer\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 1: Text Transformation & Embedding Comparison\n",
    "     We will focus on the \"transformation\" phaseâ€”moving from raw text to numerical vectors using our three distinct engines.\n",
    "     Word2Vec and FastText mathematically require the text to be split into a list of words (tokens). Our EmbeddingEngine handles\n",
    "     this internally using .split()"
   ],
   "id": "d43a435d4da8cb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Task 1: Transform the Data\n",
    "print(\"--- Task 1: Starting Data Transformation ---\")\n",
    "\n",
    "# 1. Initialize  Managers\n",
    "# Ensure 'bbc-text.csv' is in the Datasets folder relative to the project root\n",
    "bbc_path = '../Datasets/bbc_news_test.csv'\n",
    "manager = DatasetManager(bbc_path)\n",
    "embedder = EmbeddingEngine(vector_size=100)\n",
    "\n",
    "# 2. Load the Raw Data\n",
    "datasets = manager.prepare_data()\n",
    "\n",
    "# 3. Transformation & Storage\n",
    "# We will store the results in a dictionary to compare shapes/dimensions\n",
    "embedding_results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nTransforming {name.upper()} dataset...\")\n",
    "\n",
    "    # Choose the raw text column\n",
    "    # In the BBC dataset, it's usually 'text' or 'Text'\n",
    "    raw_text = df['text'] if 'text' in df.columns else df['Text']\n",
    "\n",
    "    # Generate the 3 types of embeddings\n",
    "    tfidf_vectors = embedder.get_tfidf_embeddings(raw_text)\n",
    "    w2v_vectors   = embedder.get_word2vec_embeddings(raw_text)\n",
    "    ft_vectors    = embedder.get_fasttext_embeddings(raw_text)\n",
    "\n",
    "    embedding_results[name] = {\n",
    "        'TF-IDF': tfidf_vectors,\n",
    "        'Word2Vec': w2v_vectors,\n",
    "        'FastText': ft_vectors\n",
    "    }\n",
    "\n",
    "# 4. Comparison Summary\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EMBEDDING COMPARISON (Feature Shapes)\")\n",
    "print(\"=\"*40)\n",
    "for ds_name, vectors in embedding_results.items():\n",
    "    print(f\"\\nDataset: {ds_name.upper()}\")\n",
    "    for model_name, data in vectors.items():\n",
    "        print(f\" - {model_name:10}: Shape {data.shape} (Rows, Features)\")"
   ],
   "id": "758ebe7b143b89aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### TF-IDF Shape:\n",
    "     We notice a very high number of features. This is because it creates a column for every unique word.\n",
    "#### Word2Vec/FastText Shape:\n",
    "     We see a consistent shape (e.g., (2225, 100)). This is because these are \"Dense\" embeddings where we've compressed the\n",
    "     meaning of the document into a fixed-width 100-dimension vector.\n",
    "\n",
    "This output immediately proves why PCA is much more important for the TF-IDF vectors than the neural ones.\n",
    "\n",
    "---\n",
    "\n",
    "#### Task 2: Apply Clustering Algorithms\n",
    "     We take the vectors created in Task 1 and pass them through our clustering algorithms.Because we built the Clusterer class\n",
    "     with a unified .run() method, we can systematically test every combination using a nested loop."
   ],
   "id": "87ce10e4d4503e87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Task 2: Apply Clustering Algorithms\n",
    "print(\"--- Task 2: Applying Clustering Algorithms ---\")\n",
    "\n",
    "# 1. Initialize our Clusterer\n",
    "cluster_engine = Clusterer()\n",
    "\n",
    "# 2. Results Container\n",
    "# We store these in a list of dictionaries to make evaluation in Task 3 easy\n",
    "experiment_data = []\n",
    "\n",
    "for ds_name, embeddings in embedding_results.items():\n",
    "    print(f\"\\nProcessing Dataset: {ds_name.upper()}\")\n",
    "\n",
    "    # Retrieve Ground Truth info to set 'k' (n_clusters)\n",
    "    df = datasets[ds_name]\n",
    "    if ds_name == 'bbc':\n",
    "        # Use 'category' or 'Category' depending on CSV headers\n",
    "        label_col = 'category' if 'category' in df.columns else 'Category'\n",
    "        true_labels = pd.factorize(df[label_col])[0]\n",
    "    else:\n",
    "        true_labels = df['label']\n",
    "\n",
    "    n_k = len(set(true_labels))\n",
    "    print(f\"Targeting {n_k} clusters based on ground truth labels.\")\n",
    "\n",
    "    for embed_name, X in embeddings.items():\n",
    "        for algo_name in ['kmeans', 'agglomerative', 'hdbscan']:\n",
    "            print(f\" - Running {algo_name} on {embed_name}...\")\n",
    "\n",
    "            # Execute Clustering\n",
    "            # Note: HDBSCAN will ignore n_clusters internally as per our class design\n",
    "            preds = cluster_engine.run(algo_name, X, n_clusters=n_k)\n",
    "\n",
    "            # Save the state for Task 3 (Evaluation)\n",
    "            experiment_data.append({\n",
    "                'Dataset': ds_name,\n",
    "                'Embedding': embed_name,\n",
    "                'Algorithm': algo_name,\n",
    "                'Features': X,\n",
    "                'True_Labels': true_labels,\n",
    "                'Predicted_Labels': preds\n",
    "            })\n",
    "\n",
    "print(\"\\n--- Task 2 Complete: All combinations clustered ---\")"
   ],
   "id": "5c0a0126b44f6a53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Selection of Representative Paradigms\n",
    "We didn't just pick three random algorithms; we picked three different philosophies of clustering:\n",
    "#### K-Means (Centroid-based):\n",
    "     It assumes clusters are spherical and equal in size. It's the \"baseline\" for efficiency.\n",
    "#### Agglomerative (Hierarchical):\n",
    "     It builds a tree of relationships. It's excellent for seeing if \"Sport\" and \"Business\" news share a branch before splitting.\n",
    "#### HDBSCAN (Density-based):\n",
    "     Unlike the others, it doesn't force every point into a cluster. If an article is \"weird\" or doesn't fit, HDBSCAN labels it as noise (-1). This is more\n",
    "     realistic for real-world news.\n",
    "\n",
    "### 2. Parameterization Strategy\n",
    "    For K-Means and Agglomerative, we explicitly passed the number of clusters ($k$) from our ground-truth labels (5 for BBC, 20 for 20News). This\n",
    "    allows us to measure how well the mathematical groupings align with human categories when the \"playing field\" is level.\n",
    "### HDBSCAN Decision:\n",
    "     We allowed HDBSCAN to discover the number of clusters on its own. If it finds only 2 clusters when there are actually 5, that tells us something important\n",
    "      about the \"density\" of your word embeddings.\n",
    "By looping through every Embedding + Algorithm combination, we can identify \"Winning Pairs.\" For example, you might find that TF-IDF works best with K-Means, but Word2Vec performs significantly better with Agglomerative clustering.\n",
    "\n",
    "---\n",
    "\n",
    "#### Task 3: Evaluate and Interpret Results."
   ],
   "id": "5bd89880fae149c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "688bd180006a22ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
