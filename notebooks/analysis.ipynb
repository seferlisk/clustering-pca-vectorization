{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### How the pipeline flows\n",
    " 1. Iterative Benchmarking: The nested loops ensure we test every combination (e.g., BBC + FastText + HDBSCAN, 20News +\n",
    "    TF-IDF + K-Means, etc.).\n",
    " 2. Dynamic Label Handling: For the BBC dataset, we use pd.factorize to turn text categories (like \"Sport\") into numbers.\n",
    "    For 20NewsGroups, we use the pre-existing target labels.\n",
    " 3. Visualization on the Fly: The script automatically applies the PCA → t-SNE pipeline we built to generate a 2D map for\n",
    "    every embedding/clustering combination and saves them to your outputs/ folder.\n",
    " 4. The \"Leaderboard\": At the very end, you get a clean CSV and a bar chart showing which \"Winning Pair\" had the highest NMI score."
   ],
   "id": "8357c70aa878da2b"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "#### Embedding Engine Key Technical Design Choices\n",
    "    Averaging Strategy: Since K-Means and HDBSCAN need a single vector per document, but Word2Vec/FastText produce a vector per word,\n",
    "    we calculate the Centroid (mean) of all word vectors in a document.\n",
    "\n",
    "    The Sub-word Advantage: FastText is specifically included because it handles the 20NewsGroups dataset's typos and technical jargon\n",
    "    better than Word2Vec by looking at character n-grams.\n",
    "\n",
    "    Dimensionality Note: TF-IDF usually results in thousands of dimensions (sparse), while Word2Vec/FastText are dense (set to 100\n",
    "    dimensions here). This difference is exactly why we'll need PCA later to level the playing field."
   ],
   "id": "8f1d6ee22fe5f114"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Clusterer details\n",
    "    Technical Strategy: The Density Problem\n",
    "    Notice a small but important detail in the code: HDBSCAN might return -1 for some data points. In the world of density-based\n",
    "    clustering, -1 means \"Noise\" (outliers that don't fit into any cluster).\n",
    "    When we evaluate this later, we’ll have to decide how to handle these noise points so they don't break our NMI and ARI scores.\n",
    "    Usually, we treat them as their own \"noise cluster.\""
   ],
   "id": "8ef3d28efa6fd29c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Visualizer details\n",
    "     Why did we choose the \"PCA → t-SNE\" combination?\n",
    "       We don't just jump straight into t-SNE. Here is the reasoning for this two-step pipeline:\n",
    "       - Noise Reduction: TF-IDF might have 5,000 features. t-SNE is computationally expensive and struggles with high noise. PCA\n",
    "       squashes those 5,000 down to the top 50 most \"meaningful\" directions.\n",
    "       - Global vs. Local Structure: PCA is great at keeping the big picture (global), while t-SNE is a wizard at finding local clusters.\n",
    "       Combining them gives you the best of both worlds.\n",
    "\n",
    "Handling the Comparison:\n",
    "By the time we run our experiments, the ResultStore will have a DataFrame that stores our metrics (NMI, ARI).\n",
    "Our plot_comparison method will turn this table into a grouped bar chart, making it immediately obvious which combination \"won\" for each dataset."
   ],
   "id": "7bd9a79f83831f17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Understanding the Results**\n",
    "\n",
    "When you run the project and look at the `outputs/results.csv`, keep these \"rules of thumb\" in mind:\n",
    "\n",
    "* **TF-IDF + K-Means:** Often the baseline \"king\" for news articles because news categories are heavily defined by specific keywords (e.g., \"goal\" in sports).\n",
    "* **FastText + HDBSCAN:** Usually wins on noisy data (like 20NewsGroups) because FastText handles sub-word nuances and HDBSCAN ignores \"noise\" articles that don't belong anywhere.\n",
    "* **The Silhouette Score:** If your NMI is high but Silhouette is low, it means your clusters are logically correct (aligned with the labels) but geometrically \"messy\" or overlapping in the vector space."
   ],
   "id": "ce42da0e82f099ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4f90b11505067fc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
